{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Линейный методы классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Описание:**\n",
    "Есть два класса $Y = \\{-1, +1\\}$, по обучающей выборке $X^l = (x_i, y_i)^l_{i=1}$ сделать алгоритм классификации $a(x, w) = sign f(x, w)$ ($w$ - вектор весов (параметров))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Определения:**\n",
    "$M_i(w) = y_i \\cdot f(x_i, w)$ - отступ объекта $x_i$, если $M_i < 0$, то алгоритм ошибся.\n",
    "\n",
    "*Как минимизировать эмпирический риск?*\n",
    "1. $Q(w) = \\sum_{i=1}^l (M_i(w) < 0) \\rightarrow \\min$\n",
    "\n",
    "    * Плюсы: Понятно, лего, ясно\n",
    "    * Минусы: Если $M_i$ близко к 0, то небольшое изменение условия может сделать так, что мы определим класс неверно, однако алгоритм будет оценён как хороший.\n",
    "2. $Q(w) = \\sum_{i=1}^l \\mathscr{L}(M_i(w)) \\rightarrow \\min$, где $\\mathscr{L}$ - неотрицательная, невозрастающая, гладкая"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Примеры функций:**\n",
    "\n",
    "<img src=\"Pics/functions.png\" width=\"500\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На примере линейного классификатора\n",
    "$a(x, w) = sign(\\sum_{j=1}^n w_j \\cdot f_j(x) - w_0)$\n",
    "\n",
    "Иногда вводят $f_0 = -1$ и запись сокращается до $a(x,w) = sign(<w, f>)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Как минимизировать?*\n",
    "\n",
    "Градиентный спуск!\n",
    "\n",
    "$w^{(0)}$ - начальное приближение\n",
    "$w^{(t+1)} = w^{(t)} - \\eta \\cdot \\nabla Q(w^{(t)}) = w^{(t)} - \\eta \\cdot \\sum_{i=1}^l \\mathscr{L}^'(<w^{(t)}, x_i> \\cdot y_i) \\cdot x_i \\cdot y_i$, где $\\eta$ - градиентный шаг.\n",
    "Но, что если $l$ большое? В таком случае выбирают по одному экземпляру, закон больших чисел в целом позволяет."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Алгоритм:\n",
    "\n",
    "*Вход:*\n",
    "\n",
    "* Выборка $X^l$\n",
    "* Шаг $\\eta$\n",
    "* Параметр $\\lambda$\n",
    "\n",
    "*Ход:*\n",
    "\n",
    "1. Инициализация $w^{(0)}$\n",
    "2. Инициализировать оценку функционала $Q = \\sum_{i=1}^l \\mathscr{L}(<w, x_i> \\cdot y_i)$\n",
    "3. До тех пор, пока $Q$ и / или $w$ не стабилизируются повторяем\n",
    "    1. Выбрать $x_i$ из $X^l$\n",
    "    2. Вычислить $\\epsilon_i = \\mathscr{L}(<w, x_i> \\cdot y_i)$\n",
    "    3. $w = w - \\eta \\cdot \\mathscr{L}^'(<w, x_i> \\cdot y_i) \\cdot x_i \\cdot y_i$\n",
    "    4. Оценка значения $Q$. Но как? Если мы будем пересчитывать по всей выборке, то у нас отвалится жопа!\n",
    "        Решение: $Q = (1 - \\lambda) \\cdot Q + \\lambda \\cdot \\epsilon_i$. Идея в том, что мы учитываем вес каждого объекта, но предыдущий с меньшим весом. Если новый $Q$ будет больше, уменьшаем $\\eta$\n",
    "\n",
    "*Выход:*\n",
    "\n",
    "* Вектор $w$\n",
    "\n",
    "Что значит до тех пор, пока не стабилизируется? - Пока изменение не будет незначительным."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Из модификаций можно выделить [правило Хэбба и доказывающую его теорему Новикова](http://www.machinelearning.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%9D%D0%BE%D0%B2%D0%B8%D0%BA%D0%BE%D0%B2%D0%B0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Инициализация весов\n",
    "\n",
    "* $w^{(0)} = \\{0...0\\}$\n",
    "* Небольшие случайные числа $random(-\\frac{1}{2 \\cdot n}, \\frac{1}{2 \\cdot n})$. Небольшие, так как для $a$ в основном используют сигмоиды, и если мы попадём в области горизонтальный асимптот, то производные станут равны 0.\n",
    "* $w_j = \\frac{<y, f_j>}{<f_j, f_j>}$, $f_j = (f_j(x_i))_{i=1}^l$ - вектор значений признака. Берётся из решения задачи $\\sum_{i=1}^l (<w, x_i> - y_i)^2 \\rightarrow min$\n",
    "* Обучение на подвыборке."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Порядок предъявления объектов\n",
    "\n",
    "* Перетасовка объектов\n",
    "* Чаще брать те объекты на которых допущена большая ошибка\n",
    "* Не брать объекты у которых $M_i > \\mu_i+$ - хорошие объекты\n",
    "* Не брать объекты у которых $M_i < \\mu_i-$ - объекты шумы"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Градиентный шаг\n",
    "\n",
    "* $\\eta_t \\rightarrow 0$, например $\\eta_t = \\frac{1}{t}$\n",
    "* Метод скорейшего градиентного спуска: $Q(w - \\eta \\nabla Q(w)) \\rightarrow \\min_{\\eta}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Достоинства**\n",
    "\n",
    "* Легко реализовать\n",
    "* Обобщается на любые $f, \\mathscr{L}$\n",
    "* На сверхбольших выборках не обязательно брать все $x_i$\n",
    "\n",
    "**Недостатки:**\n",
    "\n",
    "* Возможна расходимость или медленная сходимость\n",
    "* Застревание в локальных минимумах\n",
    "* Подбор начальных параметров\n",
    "* Переобучение"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Проблема переобучения\n",
    "\n",
    "**Причины:**\n",
    "\n",
    "1. Слишком мало объектов, признаков.\n",
    "2. Линейная зависимость признаков. Тогда существует не нулевой $u \\in \\mathcal{R}^{n+1} : <u, x> = 0$, а значит $\\forall \\gamma \\in mathcal{R} a(x,w) = sign<w + \\gamma \\cdot u, x>$\n",
    "\n",
    "**Последствия:**\n",
    "\n",
    "1. Слишком большие веса $||w||$\n",
    "2. Неустойчивость $a(x,w)$\n",
    "3. $Q(X^l) << Q(X^k)$\n",
    "\n",
    "**Решения:**\n",
    "\n",
    "1. Сокращение весов\n",
    "2. Раний останов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Регуляризация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Многие из признаков являются \"шумовыми\" или линейной комбинацией других, поэтому приходит идея о регуляризации, где мы фактически будем минимизировать $C \\sum_{i=1}^l (1 - M_i(w, w_0)) + \\mu \\sum_{i=1}^l f(w_i)$ (где $f$ монотонная верх), тогда при увеличении $\\mu$ будем выгодно уменьшать $w_i$ и тем самым отбрасывать ненужные признаки"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Примеры"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Квадратичный (гауссовский) регуляризатор"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$p(w, \\sigma) = \\frac{1}{(2 \\cdot \\pi \\sigma)^{\\frac{n}{2}} \\cdot exp(-\\frac{||w||^2}{2 \\cdot \\sigma})$\n",
    "\n",
    "Тогда $-\\ln(p(w, \\sigma)) = \\frac{1}{2 \\cdot \\sigma} \\cdot ||w||^2 + const(w)$\n",
    "\n",
    "$\\tau = \\frac{1}{sigma}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Лапласовский регуляризатор"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$p(w, C) = \\frac{1}{(2C)^n)} \\cdot exp(-\\frac{||w||_1}{C})$\n",
    "\n",
    "Тогда $-\\ln(p(w, C)) = \\frac{1}{C} \\cdot \\sum_{j=1}^n |w_j| + const$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}