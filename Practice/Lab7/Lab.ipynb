{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "file = \"sabatini-r.-odisseya-kapitana-blada-getlib.ru.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "Sequence_length = 20\n",
    "\n",
    "New_chapter = re.compile(\"^\\d+\\.\")\n",
    "Unnecessary = [':', '\\\"', '\\'', '(', ')', ',', ':', '-', '_', '/', '«', '—', '–', '»', '[', ']', '„', '“',\n",
    "               '&', '#', '#']\n",
    "End_symbols = ['?', ';', '!', '..', '…']\n",
    "End_symbol = '.'\n",
    "\n",
    "\n",
    "def encode_symbol(symbol: str, encoder: dict[str, int]) -> list[int]:\n",
    "    encoded_symbol = [0] * len(encoder)\n",
    "    encoded_symbol[encoder[symbol]] = 1\n",
    "    return encoded_symbol\n",
    "\n",
    "\n",
    "def encode_string(string: str, encoder: dict[str, int]) -> list[list[int]]:\n",
    "    encoded_string = []\n",
    "    for symbol in string:\n",
    "        encoded_string.append(encode_symbol(symbol, encoder))\n",
    "    return encoded_string\n",
    "\n",
    "\n",
    "def encode_data(data: list[str]) -> tuple[list[list[list[int]]], dict[str, int], dict[int, str]]:\n",
    "    symbols = list(set([symbol for string in data for symbol in string]))\n",
    "    count_symbols = len(symbols)\n",
    "    encoder = dict(zip(symbols, range(count_symbols)))\n",
    "    encoded_data = []\n",
    "    for string in tqdm(data, desc=\"Encode sentences\"):\n",
    "        encoded_data.append(encode_string(string, encoder))\n",
    "    return encoded_data, encoder, {v: k for k, v in encoder.items()}\n",
    "\n",
    "\n",
    "def load_data(file_name: str):\n",
    "    data = []\n",
    "    text = \"\"\n",
    "    dataset_file = codecs.open(file_name, 'r', encoding='utf_8_sig')\n",
    "    for line in dataset_file.readlines():\n",
    "        line = line.strip()\n",
    "        if (len(line) == 0) or line.isupper() or New_chapter.match(line):\n",
    "            continue\n",
    "\n",
    "        line = line.lower()\n",
    "        for char in Unnecessary:\n",
    "            line = line.replace(char, ' ')\n",
    "        for char in End_symbols:\n",
    "            line = line.replace(char, End_symbol)\n",
    "        line = \" \".join(line.split())\n",
    "        lines = line.split(sep='.')\n",
    "        for l in lines:\n",
    "            l = l.strip()\n",
    "            if len(l) == 0:\n",
    "                continue\n",
    "            data.append(l + End_symbol)\n",
    "            text += l + End_symbol\n",
    "    dataset_file.close()\n",
    "    return data, *encode_data(data), text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encode sentences: 100%|██████████| 7768/7768 [00:01<00:00, 5112.73it/s]\n"
     ]
    }
   ],
   "source": [
    "input_sentences, encoded_sentences, encoder_map, decoder_map, input_text = load_data(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'рафаэль сабатини.'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'p',\n 1: 'i',\n 2: '4',\n 3: '2',\n 4: 'и',\n 5: '0',\n 6: 'ш',\n 7: 'к',\n 8: 'з',\n 9: 'щ',\n 10: 'й',\n 11: 'r',\n 12: 'v',\n 13: 'о',\n 14: 'м',\n 15: '′',\n 16: 'д',\n 17: '7',\n 18: 't',\n 19: 'г',\n 20: 'n',\n 21: 'u',\n 22: 'h',\n 23: '3',\n 24: 'т',\n 25: 'a',\n 26: '.',\n 27: 'j',\n 28: 'у',\n 29: 'п',\n 30: 'm',\n 31: 'е',\n 32: 'а',\n 33: 'х',\n 34: '6',\n 35: 'с',\n 36: '8',\n 37: 'd',\n 38: 'б',\n 39: '9',\n 40: 'o',\n 41: '1',\n 42: 'b',\n 43: 'н',\n 44: 'ь',\n 45: 'ъ',\n 46: 'л',\n 47: '5',\n 48: 'c',\n 49: 'x',\n 50: '́',\n 51: 'ю',\n 52: 'f',\n 53: 'э',\n 54: 'я',\n 55: 'ё',\n 56: 'q',\n 57: 'в',\n 58: 'ч',\n 59: 'ы',\n 60: '°',\n 61: 'ц',\n 62: ' ',\n 63: 'e',\n 64: 's',\n 65: 'l',\n 66: 'ж',\n 67: 'ф',\n 68: 'g',\n 69: 'р'}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "{'p': 0,\n 'i': 1,\n '4': 2,\n '2': 3,\n 'и': 4,\n '0': 5,\n 'ш': 6,\n 'к': 7,\n 'з': 8,\n 'щ': 9,\n 'й': 10,\n 'r': 11,\n 'v': 12,\n 'о': 13,\n 'м': 14,\n '′': 15,\n 'д': 16,\n '7': 17,\n 't': 18,\n 'г': 19,\n 'n': 20,\n 'u': 21,\n 'h': 22,\n '3': 23,\n 'т': 24,\n 'a': 25,\n '.': 26,\n 'j': 27,\n 'у': 28,\n 'п': 29,\n 'm': 30,\n 'е': 31,\n 'а': 32,\n 'х': 33,\n '6': 34,\n 'с': 35,\n '8': 36,\n 'd': 37,\n 'б': 38,\n '9': 39,\n 'o': 40,\n '1': 41,\n 'b': 42,\n 'н': 43,\n 'ь': 44,\n 'ъ': 45,\n 'л': 46,\n '5': 47,\n 'c': 48,\n 'x': 49,\n '́': 50,\n 'ю': 51,\n 'f': 52,\n 'э': 53,\n 'я': 54,\n 'ё': 55,\n 'q': 56,\n 'в': 57,\n 'ч': 58,\n 'ы': 59,\n '°': 60,\n 'ц': 61,\n ' ': 62,\n 'e': 63,\n 's': 64,\n 'l': 65,\n 'ж': 66,\n 'ф': 67,\n 'g': 68,\n 'р': 69}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 1],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 1, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(encoded_sentences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cut sentences: 100%|██████████| 7768/7768 [00:00<00:00, 14709.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "((424984, 20, 70), (424984, 70))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(sentences, length=Sequence_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for sentence in tqdm(sentences, desc=\"Cut sentences\"):\n",
    "        if len(sentence) <= length:\n",
    "            continue\n",
    "        for i in range(0, len(sentence) - length):\n",
    "            x = sentence[i:i + length]\n",
    "            y = sentence[i + length]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "data_x, data_y = get_data(encoded_sentences)\n",
    "data_x.shape, data_y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "Epochs = 30\n",
    "\n",
    "def create_lstm_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(128, input_shape=(data_x.shape[1], data_x.shape[2])),\n",
    "        tf.keras.layers.Dense(data_y.shape[1], activation='softmax')]\n",
    "    )\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_lstm_model(model, x, y):\n",
    "    model.fit(x, y, epochs=Epochs, verbose=1)\n",
    "\n",
    "\n",
    "def sample(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    probabilities = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "\n",
    "def predict_lstm_model(model, start_string, encoder, decoder):\n",
    "    print(start_string, end=\"\")\n",
    "    start_string = encode_string(start_string, encoder)\n",
    "    while True:\n",
    "        prediction = model.predict(np.array([start_string]), verbose=0)\n",
    "        index = sample(prediction[0], 0.5)\n",
    "        result = decoder[index]\n",
    "        print(result, end=\"\")\n",
    "        if result == End_symbol:\n",
    "            return\n",
    "        start_string.append(encode_symbol(result, encoder))\n",
    "        start_string = start_string[1:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "13281/13281 [==============================] - 143s 11ms/step - loss: 2.3464\n",
      "Epoch 2/30\n",
      "13281/13281 [==============================] - 167s 13ms/step - loss: 1.9725\n",
      "Epoch 3/30\n",
      "13281/13281 [==============================] - 139s 10ms/step - loss: 1.8156\n",
      "Epoch 4/30\n",
      "13281/13281 [==============================] - 146s 11ms/step - loss: 1.7192\n",
      "Epoch 5/30\n",
      "13281/13281 [==============================] - 156s 12ms/step - loss: 1.6534\n",
      "Epoch 6/30\n",
      "13281/13281 [==============================] - 149s 11ms/step - loss: 1.6062\n",
      "Epoch 7/30\n",
      "13281/13281 [==============================] - 153s 12ms/step - loss: 1.5698\n",
      "Epoch 8/30\n",
      "13281/13281 [==============================] - 156s 12ms/step - loss: 1.5407\n",
      "Epoch 9/30\n",
      "13281/13281 [==============================] - 154s 12ms/step - loss: 1.5175\n",
      "Epoch 10/30\n",
      "13281/13281 [==============================] - 155s 12ms/step - loss: 1.4978\n",
      "Epoch 11/30\n",
      "13281/13281 [==============================] - 169s 13ms/step - loss: 1.4812\n",
      "Epoch 12/30\n",
      "13281/13281 [==============================] - 172s 13ms/step - loss: 1.4665\n",
      "Epoch 13/30\n",
      "13281/13281 [==============================] - 176s 13ms/step - loss: 1.4539\n",
      "Epoch 14/30\n",
      "13281/13281 [==============================] - 174s 13ms/step - loss: 1.4426\n",
      "Epoch 15/30\n",
      "13281/13281 [==============================] - 169s 13ms/step - loss: 1.4329\n",
      "Epoch 16/30\n",
      "13281/13281 [==============================] - 167s 13ms/step - loss: 1.4238\n",
      "Epoch 17/30\n",
      "13281/13281 [==============================] - 166s 12ms/step - loss: 1.4150\n",
      "Epoch 18/30\n",
      "13281/13281 [==============================] - 169s 13ms/step - loss: 1.4071\n",
      "Epoch 19/30\n",
      "13281/13281 [==============================] - 173s 13ms/step - loss: 1.4004\n",
      "Epoch 20/30\n",
      "13281/13281 [==============================] - 183s 14ms/step - loss: 1.3938\n",
      "Epoch 21/30\n",
      "13281/13281 [==============================] - 184s 14ms/step - loss: 1.3879\n",
      "Epoch 22/30\n",
      "13281/13281 [==============================] - 191s 14ms/step - loss: 1.3824\n",
      "Epoch 23/30\n",
      "13281/13281 [==============================] - 194s 15ms/step - loss: 1.3776\n",
      "Epoch 24/30\n",
      "13281/13281 [==============================] - 196s 15ms/step - loss: 1.3729\n",
      "Epoch 25/30\n",
      "13281/13281 [==============================] - 211s 16ms/step - loss: 1.3683\n",
      "Epoch 26/30\n",
      "13281/13281 [==============================] - 220s 17ms/step - loss: 1.3642\n",
      "Epoch 27/30\n",
      "13281/13281 [==============================] - 216s 16ms/step - loss: 1.3601\n",
      "Epoch 28/30\n",
      "13281/13281 [==============================] - 229s 17ms/step - loss: 1.3563\n",
      "Epoch 29/30\n",
      "13281/13281 [==============================] - 236s 18ms/step - loss: 1.3531\n",
      "Epoch 30/30\n",
      "13281/13281 [==============================] - 226s 17ms/step - loss: 1.3495\n"
     ]
    }
   ],
   "source": [
    "lstm = create_lstm_model()\n",
    "fit_lstm_model(lstm, data_x, data_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "капитан блад поднял его с долго на который вновь проблизнился к нему из голландся губернатора появилась и всё же не можете оказаться он был головой ему приказал по себе на свою капитан блад и в бледном кораблей и вернулся и ответила и что объяснялись взять этого лица вы не насмешки и не заметил блад и просто никакого придум на него сказал собраться с ним и прибыл к капитан блад прибыли находился в сторону обосяк принял их по испански и добавил на рассказал его вы намереваясь на него с подавающих насторилась на него отправляет с них привести потому что был под того как до того как был только до того что он не задержался в себя и быть с тем погладела в барбадос а в таком судеать своего как не приняла не сказать ваш не была повесить и это сильными голландских море до того своего подвергаться к нему стали одно и добраться до корабль который был на борту и назначилась от бросили раздражали пристально заметно в которых он не раздражённое высокий ваших полученными на службу к берегу с заключающий местно на полученных из ваша честь всего действовать его своим делать его замечательно прибытии заполным начал вы безопасности корабля на него солнца почти принять до офицерам прибыть в полковник бишоп вы случиться на этим сух поклонение у вашем заметил что он более что получил на море он и что они более из лицо всю манцивалось рассчатил что в таком соробными в обстоятельства хотя в его светлость делать что по совете направлялись в этих судья погладел на прибый дон мигель его скромно придала в ваших признатое прибыл вам предложение поблагодарил блад."
     ]
    }
   ],
   "source": [
    "start = \"капитан блад поднял\"\n",
    "predict_lstm_model(lstm, start, encoder_map, decoder_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "Suffix_length = 4\n",
    "\n",
    "def create_markov_model(text):\n",
    "    counts = dict()\n",
    "    for i in tqdm(range(0, len(text) - Suffix_length - 1), desc=\"Read sentences\"):\n",
    "        cur_symbols = text[i: i + Suffix_length]\n",
    "        next_symbols = text[i + 1: i + Suffix_length + 1]\n",
    "        if cur_symbols not in counts:\n",
    "            counts[cur_symbols] = dict()\n",
    "        if next_symbols not in counts[cur_symbols]:\n",
    "            counts[cur_symbols][next_symbols] = 0\n",
    "        counts[cur_symbols][next_symbols] += 1\n",
    "    probabilities = dict()\n",
    "    for cur, dict_next in tqdm(counts.items(), desc=\"Count probabilities\"):\n",
    "        next_to_int = dict()\n",
    "        next_probabilities = []\n",
    "        next_number = 0\n",
    "        counts_sum = sum(dict_next.values())\n",
    "        for next_symbols, count in dict_next.items():\n",
    "            next_to_int[next_symbols] = next_number\n",
    "            next_number += 1\n",
    "            next_probabilities.append(count / counts_sum)\n",
    "        probabilities[cur] = (next_probabilities, next_to_int, {v: k for k, v in next_to_int.items()})\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def predict_markov(model, string):\n",
    "    if string not in model:\n",
    "        print('No such text in Markov model')\n",
    "        return '.'\n",
    "\n",
    "    next_symbols, next_to_int, int_to_next = model[string]\n",
    "    index = sample(next_symbols, 0.5)\n",
    "    return int_to_next[index][-1]\n",
    "\n",
    "\n",
    "def predict_markov_model(model, start_string):\n",
    "    print(start_string, end=\"\")\n",
    "    start_string = start_string[-Suffix_length:]\n",
    "    while True:\n",
    "        prediction = predict_markov(model, start_string)\n",
    "        if prediction is None:\n",
    "            return\n",
    "        print(prediction, end=\"\")\n",
    "        if prediction == End_symbol:\n",
    "            return\n",
    "        start_string += prediction\n",
    "        start_string = start_string[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read sentences: 100%|██████████| 572446/572446 [00:00<00:00, 953448.66it/s]\n",
      "Count probabilities: 100%|██████████| 38959/38959 [00:00<00:00, 371981.49it/s]\n"
     ]
    }
   ],
   "source": [
    "markov_model = create_markov_model(input_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тут лорд джулиан сказал он сказал он под после этого подошёл вслед за чтобы находит что уважением обратил внимать мои людей по том что вашей случай мне как со мне как он соответил блада сказал он."
     ]
    }
   ],
   "source": [
    "predict_markov_model(markov_model, 'тут л')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}